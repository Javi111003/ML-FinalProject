\documentclass[12pt,a4paper]{article}

% Paquetes útiles
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{hyperref}   % Para hipervínculos en el índice
\usepackage{graphicx}   % Para imágenes
\usepackage{amsmath}    % Para fórmulas matemáticas
\usepackage{cite}       % Para citas en la bibliografía
\usepackage{float}
\usepackage[acronym]{glossaries} 

\makeglossaries

\newglossaryentry{ml}{name={ML (Machine Learning)},description={Conjunto de métodos que permiten a un sistema aprender patrones a partir de datos sin programación explícita}} 
\newglossaryentry{dl}{name={DL (Deep Learning)},description={Subcampo de Machine Learning basado en redes neuronales profundas con múltiples capas ocultas}}
\newglossaryentry{mlclasico}{name={ML clásico},description={Algoritmos de Machine Learning no profundos, como k-NN, SVM, Árboles de decisión y modelos estadísticos}} 
\newglossaryentry{dnavanzado}{name={DL avanzado},description={Arquitecturas profundas modernas como Transformers, GNNs y modelos híbridos}} 
\newglossaryentry{arma}{name={ARMA},description={Modelo estadístico para series temporales que combina componentes autorregresivos y de media móvil}} 
\newglossaryentry{arima}{name={ARIMA},description={Extensión de ARMA que incorpora diferenciación para manejar series no estacionarias}} 
\newglossaryentry{sarima}{name={SARIMA},description={Modelo ARIMA que incluye explícitamente componentes estacionales}} 
\newglossaryentry{holtwinters}{name={Holt-Winters},description={Método de suavizado exponencial que modela nivel, tendencia y estacionalidad}} 
\newglossaryentry{prophet}{name={Prophet},description={Modelo aditivo para series temporales desarrollado por Meta, robusto ante estacionalidades múltiples}} 
\newglossaryentry{mlp}{name={MLP (Multi-Layer Perceptron)},description={Red neuronal feed-forward clásica compuesta por múltiples capas totalmente conectadas}} 
\newglossaryentry{dnn}{name={DNN (Deep Neural Network)},description={Red neuronal profunda con varias capas ocultas}} 
\newglossaryentry{rnn}{name={RNN (Recurrent Neural Network)},description={Red neuronal diseñada para procesar datos secuenciales}} 
\newglossaryentry{lstm}{name={LSTM (Long Short-Term Memory)},description={Tipo de RNN capaz de capturar dependencias de largo plazo en series temporales}} 
\newglossaryentry{gru}{name={GRU (Gated Recurrent Unit)},description={Variante simplificada de LSTM con menor número de parámetros}} 
\newglossaryentry{rclstm}{name={RCLSTM},description={LSTM de complejidad reducida o dispersa que mantiene rendimiento con menos parámetros}} 
\newglossaryentry{cnn}{name={CNN (Convolutional Neural Network)},description={Red neuronal especializada en la extracción de patrones espaciales}} 
\newglossaryentry{cnn2d}{name={CNN 2D},description={CNN aplicada a representaciones matriciales espacio-temporales del tráfico}} 
\newglossaryentry{cnn+lstm}{name={CNN + LSTM},description={Modelo híbrido donde CNN captura correlaciones espaciales y LSTM dependencias temporales}} 
\newglossaryentry{gnn}{name={GNN (Graph Neural Network)},description={Red neuronal diseñada para datos estructurados como grafos}} 
\newglossaryentry{gcn}{name={GCN (Graph Convolutional Network)},description={Tipo de GNN que generaliza la operación de convolución a grafos}} 
\newglossaryentry{transformer}{name={Transformer},description={Arquitectura basada en mecanismos de atención que captura dependencias globales de largo alcance}} 
\newglossaryentry{tcn}{name={TCN (Temporal Convolutional Network)},description={Red convolucional causal diseñada para modelado temporal}} 
\newglossaryentry{knn}{name={K-NN (K-Nearest Neighbors)},description={Algoritmo de aprendizaje basado en instancias que utiliza vecinos más cercanos}} 
\newglossaryentry{svm}{name={SVM (Support Vector Machine)},description={Algoritmo de clasificación y regresión basado en maximización del margen}} 
\newglossaryentry{svr}{name={SVR (Support Vector Regression)},description={Versión de SVM para tareas de regresión}} 
\newglossaryentry{sosvsvr}{name={SOS-vSVR},description={SVR optimizado mediante técnicas de optimización por enjambre}} 
\newglossaryentry{randomforest}{name={Random Forest},description={Método de ensamble basado en múltiples árboles de decisión}} 
\newglossaryentry{xgboost}{name={XGBoost},description={Algoritmo de gradient boosting eficiente y altamente competitivo}} 
\newglossaryentry{naivebayes}{name={Naive Bayes},description={Clasificador probabilístico basado en el teorema de Bayes con independencia condicional}} 
\newglossaryentry{kmeans}{name={K-means},description={Algoritmo de clustering no supervisado basado en centroides}} 
\newglossaryentry{dbscan}{name={DBSCAN},description={Algoritmo de clustering basado en densidad capaz de detectar ruido y anomalías}} 
\newglossaryentry{pca}{name={PCA (Principal Component Analysis)},description={Técnica de reducción de dimensionalidad basada en varianza}} 
\newglossaryentry{elm}{name={ELM (Extreme Learning Machine)},description={Red neuronal de una sola capa con entrenamiento extremadamente rápido}} 
\newglossaryentry{oselm}{name={OS-ELM},description={Variante de ELM para aprendizaje secuencial u online}} 
\newglossaryentry{htm}{name={HTM (Hierarchical Temporal Memory)},description={Modelo inspirado en la neocorteza para procesamiento temporal, no basado en deep learning}} 
\newglossaryentry{throughput}{name={Throughput},description={Tasa efectiva de transmisión de datos en una red}} 
\newglossaryentry{lte}{name={LTE (Long Term Evolution)},description={Estándar de comunicaciones móviles de cuarta generación (4G)}} 
\newglossaryentry{5g6g}{name={5G / 6G},description={Quinta y futura sexta generación de redes móviles}} 
\newglossaryentry{qos}{name={QoS (Quality of Service)},description={Conjunto de métricas que miden la calidad de una red (latencia, pérdida, ancho de banda)}} 
\newglossaryentry{traficoespaciotemporal}{name={Tráfico espacio-temporal},description={Modelado del tráfico considerando dimensiones espaciales y temporales}} 
\newglossaryentry{topologia}{name={Topología de red},description={Estructura de interconexión entre los nodos de una red}} 
\newglossaryentry{fl}{name={FL (Federated Learning)},description={Paradigma de aprendizaje distribuido que preserva la privacidad de los datos}} 
\newglossaryentry{aprendizajedistribuido}{name={Aprendizaje distribuido},description={Entrenamiento de modelos en múltiples nodos computacionales}} 
\newglossaryentry{mape}{name={MAPE (Mean Absolute Percentage Error)},description={Métrica de error basada en el porcentaje medio absoluto}} 
\newglossaryentry{baseline}{name={Baseline},description={Modelo de referencia utilizado para comparación de rendimiento}}


\begin{document}

% Portada
\begin{titlepage} 
    \centering 
    {\Huge \bfseries Proyecto Final de Aprendizaje de Máquina \par} 
    
    \vspace{0.5cm} 
    
    {\Large Estudio sobre registros de consumo de datos, voz y sms \par} 

    \vspace{2cm} 

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{./images/logoMATCOM.png}
    \end{figure}
    \vfill 
    % empuja el contenido hacia abajo 
    % Autores en esquina inferior izquierda 
    \begin{flushleft} 
        {\large {\textbf{Integrantes:} 

        \vspace{0.5cm} 

        Claudia Hernández Pérez\\ 
        Joel Aparicio Tamayo\\ 
        Kevin Márquez Vega\\ 
        Javier A. González Díaz\\ 
        José Miguel Leyva Cruz\\ 
        Luis E. Amat Cárdenas}} 

        \vspace{1cm} 
        
        \begin{flushright} 
            {\large{\textbf{Grupo:} C-412}} 

            \vspace{0.5cm} 

            {\large {\textbf{Tutor:} Lic. Roberto Marti Cedeño}}
        \end{flushright}
    \end{flushleft} 
    
    \vspace{1cm} 
\end{titlepage}


% Índice
\tableofcontents
\newpage

\section{Estudio del estado del arte}

La investigación sobre predicción y clasificación de tráfico en redes ha evolucionado de manera notable en los últimos quince años. 
En la primera etapa (2010--2013), los modelos estadísticos clásicos como \gls{arma}, \gls{arima} y \gls{sarima} fueron la base para el análisis de series temporales, 
mientras que las redes neuronales superficiales como \gls{mlp} demostraron capacidad para capturar patrones complejos antes del auge del \gls{dl}. 

A partir de 2014, el \gls{dl} comenzó a aplicarse con redes profundas como \gls{dnn}, mostrando mejoras claras frente a los métodos tradicionales. 
Posteriormente, los modelos híbridos que combinan \gls{cnn} y \gls{lstm} permitieron capturar tanto correlaciones espaciales como temporales, 
consolidando el enfoque de \gls{traficoespaciotemporal} en la predicción de tráfico. 

Entre 2017 y 2018 surgieron propuestas orientadas a la eficiencia y nuevas representaciones: \gls{rclstm} redujo parámetros manteniendo rendimiento, 
mientras que la representación del tráfico como imágenes espacio-temporales con \gls{cnn2d} mejoró la precisión. 
En paralelo, se exploraron integraciones más complejas como DMVST-Net, y se mantuvo la relevancia de algoritmos clásicos como \gls{svm}, \gls{knn} y árboles, en tareas de clasificación. 

En el periodo 2019--2021 se diversificaron las técnicas, incorporando clustering y reducción de dimensionalidad por ejemplo: \gls{kmeans}, \gls{dbscan} y \gls{pca}, 
además de modelos optimizados como \gls{svr} y \gls{xgboost}, que superaron a los estadísticos en escenarios no lineales. 
Las comparativas entre \gls{ml} y \gls{dl} confirmaron la superioridad de \gls{lstm}, aunque \gls{mlp} se mantuvo competitivo. 

Los avances recientes (2022--2023) destacan el uso de \gls{knn} con selección de características en \gls{lte}, 
la incorporación de grafos mediante \gls{gnn} combinados con \gls{rnn} para modelar dependencias espaciales y temporales, 
y la validación de modelos estadísticos como \gls{sarima} y \gls{holtwinters} en contextos específicos. 
También se resaltan alternativas rápidas y robustas como \gls{oselm} y \gls{randomforest}. 

Finalmente, las tendencias emergentes (2024--2025) apuntan hacia arquitecturas modernas como \gls{transformer} y \gls{tcn}, 
capaces de capturar dependencias globales y temporales con gran precisión. 
Se exploran enfoques eficientes como \gls{htm} y \gls{fl} para preservar privacidad, 
y se proponen marcos híbridos adaptativos para entornos \gls{5g6g}.
En paralelo, la detección de anomalías en tráfico de telecomunicaciones ha cobrado relevancia, con el uso de algoritmos como \gls{dbscan} y \gls{svm} \cite{DBSCAN}. 
Las revisiones recientes concluyen que el aprendizaje profundo moderno con \gls{lstm}, \gls{gnn} y \gls{transformer} domina en problemas \gls{traficoespaciotemporal} complejos, 
mientras que los métodos \gls{mlclasico} siguen siendo útiles en escenarios simples o con restricciones de recursos.

\section{Estudio sobre los datos}

La Empresa de Telecomunicaciones de Cuba S.A. (ETECSA) proporcionó un conjunto de datos con el propósito de desarrollar estudios y modelos basados en técnicas de Aprendizaje de Máquina (Machine Learning). Dichos datos reflejan el uso de diversos servicios de telecomunicaciones por parte de los usuarios, tales como llamadas telefónicas, mensajes de texto (SMS), recargas de saldo y consumo de datos móviles. 

El objetivo principal de este análisis es comprender la estructura, el contenido y las características de los datos, con vistas a su preparación y posterior aplicación en modelos predictivos o de análisis de comportamiento.

\subsection{Descripción general del conjunto de datos}
El conjunto de datos se encuentra en formato tabular y contiene aproximadamente 10 mil registros pertenecientes a 1102 usuarios 
y 40 variables, distribuidas en columnas que describen los diferentes aspectos de cada transacción o evento de uso de servicios. Los registros son sobre
el consumo de los servicios datos, sms, voz y recarga, en distinta proporción (ver Figura \ref{fig:prop}). 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{./images/proporcion_datos.png}
    \caption{Proporción de los servicios en los datos.}
    \label{fig:prop}
\end{figure}

Cada fila representa un registro detallado de uso de servicio (CDR, por sus siglas en inglés: Call Detail Record), 
que documenta información relacionada con un evento generado por el cliente, como una llamada, el envío de un mensaje o una conexión a 
internet móvil.

Los registros reportan fechas entre las 12 de la madrugada y 12 del mediodía del día 1 de octubre de 2025 en UTC (lo que serían las 8 de la noche 
del 30 de septiembre y 8 de la mañana del 1 de octubre hora de Cuba `UTC-4') del día 1 de octubre de 2025.

A continuación, se presenta un resumen de los tipos de variables más relevantes (ver Cuadro \ref{tab:desc}):

\begin{table}[H]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|p{7.5cm}|}
\hline
\textbf{Tipo de variable} & \textbf{Ejemplo de campos} & \textbf{Descripción general} \\
\hline
Identificadores & CDR\_ID, OBJ\_ID, OWNER\_CUST\_ID & Identifican de manera única cada registro, objeto o cliente asociado. \\
\hline
Temporales & START\_DATE, END\_DATE & Indican la fecha y hora de inicio y fin del servicio utilizado. \\
\hline
Categóricas & SERVICE\_CATEGORY, FLOW\_TYPE, USAGE\_SERVICE\_TYPE & Especifican el tipo de servicio, su categoría (voz, datos, SMS, recarga) y dirección del tráfico (entrante/saliente). \\
\hline
Numéricas & ACTUAL\_USAGE, ACTUAL\_CHARGE, TOTAL\_TAX\_AMOUNT & Miden el volumen de uso (minutos, megabytes, mensajes) y los cargos monetarios asociados. \\
\hline
Listas o estructuras anidadas & CHARGE\_LIST, CHARGE\_SERVICE\_INFO, BALANCE\_CHG\_LIST & Detallan cargos, impuestos y modificaciones de saldo que se producen en cada evento. \\
\hline
\end{tabular}
\caption{Descripciones de los datos.} 
\label{tab:desc}
\end{table}

Estos campos se complementan con información auxiliar relacionada con unidades de medida, identificadores de cuenta, ciclos de facturación y valores reservados para futuras ampliaciones del sistema.

\subsection{Distribución de los datos}
Con el fin de visualizar el comportamiento de los datos se muestran sus distribuciones (ver Figura \ref{fig:dist}).

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./images/distribucion_consumo.png}
    \caption{Distribución de los datos}
    \label{fig:dist}
\end{figure}

Las gráficas muestran ciertas irregularidades que resultan de interés. La intención es realizar un estudio más profundo de estas anomalías por independiente, dado que no existe ninguna relación entre ellos (ver Figura \ref{fig:corr}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./images/correlacion_servicios.png}
    \caption{Correlación entre servicios.}
    \label{fig:corr}
\end{figure}

\section{Detección de patrones}
Como mostraron las gráficas en cada uno de los servicios estudiados se observan ``picos'' que pueden describir eventos en los datos (ver Figura \ref{fig:cmp}). 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/comparacion_servicios.png}
    \caption{Comparación de consumo de servicios.}
    \label{fig:cmp}
\end{figure}

Se analizaron los comportamientos más relevantes de cada servicio utilizando \gls{dbscan} y los patrones de horario por servicio usando series de tiempo.

\subsection{Datos}

En las horas registradas en el horario entre las 4 y 6 de la madrugada (ver Figura \ref{fig:ptr_d}), se refleja un notable consumo, que pudiera estar relacionado con las promociones de recargas con datos ilimitados en el horario de la madrugada.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{./images/patrones_horarios_datos.png}
    \caption{Patrones de horario del servicio de datos.}
    \label{fig:ptr_d}
\end{figure}

Tras un análisis realizado sobre el uso del servicio en un intervalo de tiempo se observaron anomalías extremas, habían registros de muy bajo consumo pero también de muy alto. 
Esto se refleja en una notable diferencia entre la media de consumo de datos en estas anomalías y en los consumidores ``normales'' (ver Cuadro \ref{tab:actual_usage_d}).

\begin{table}[H]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|}
\hline
\textbf{Variable} & \textbf{Valor (Byte)} \\
\hline
Media Normal & 7\,853\,641.44 \\
\hline
Media Outlier & 249\,439\,482.75 \\
\hline
Min Outlier & 64.0 \\
\hline
Max Outlier & 14\,114\,331\,638.0 \\
\hline
\end{tabular}
\caption{Resumen de valores de la variable ACTUAL\_USAGE} \label{tab:actual_usage_d}
\end{table}

\subsection{Voz}

En las horas registradas se muestra un consumo significativo en el horario entre las 12 y 2 de la madrugada (ver Figura \ref{fig:ptr_v}). Este horario destaca 
debido a que los otros registros pertenecen a horas de la madrugada donde tiene sentido que disminuya el uso de llamadas.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{./images/patrones_horarios_voz.png}
    \caption{Patrones de horario del servicio de voz.}
    \label{fig:ptr_v}
\end{figure}

Se identificaron anomalías significativas en las llamadas. 
Se consideraron aquellas llamadas cuya duración superaba ampliamente la media, 
utilizando la columna \texttt{ACTUAL\_USAGE} (ver Cuadro \ref{tab:actual_usage_v}). 
Además, se examinó la variable \texttt{ACTUAL\_CHARGE} (ver Cuadro \ref{tab:actual_charge_v}), la cual mostró una irregularidad marcada: 
la mayoría de los registros presentan valores en cero, mientras que algunos alcanzan cifras superiores a 2\,000. 
Finalmente, se analizó la cantidad de llamadas recibidas por los usuarios (ver Cuadro \ref{tab:rec_calls_v})

\begin{table}[H]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|}
\hline
\textbf{Variable} & \textbf{Valor (min)} \\
\hline
Media Normal & 36.52 \\
\hline
Media Outlier & 310.5 \\
\hline
Min Outlier & 97.0 \\
\hline
Max Outlier & 831.0 \\
\hline
\end{tabular}
\caption{Resumen de valores de la variable ACTUAL\_USAGE.} 
\label{tab:actual_usage_v}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|}
\hline
\textbf{Variable} & \textbf{Valor (centavos)} \\
\hline
Media Normal & 0.0 \\
\hline
Media Outlier & 18\,888.89 \\
\hline
Min Outlier & 2\,500.0 \\
\hline
Max Outlier & 80\,500.0 \\
\hline
\end{tabular}
\caption{Resumen de valores de la variable ACTUAL\_CHARGE.} 
\label{tab:actual_charge_v}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|}
\hline
\textbf{Variable} & \textbf{Valor} \\
\hline
Media Normal & 1.20 \\
\hline
Media Outlier & 5.0 \\
\hline
Min Outlier & 4 \\
\hline
Max Outlier & 6 \\
\hline
\end{tabular}
\caption{Resumen de valores de la variable llamadas recibidas.} 
\label{tab:rec_calls_v}
\end{table}

\subsection{SMS}

En las horas registradas se observa un pequeño aumento del consumo en el horario entre las 6 y 8 de la mañana (ver Figura \ref{fig:ptr_s}), lo que pudiera estar dado debido 
a que a esas horas la comunicación por mensajes es más discreta que realizar una llamada. No obstante no se distinguen diferencias significativas.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{./images/patrones_horarios_sms.png}
    \caption{Patrones de horario del servicio de sms.}
    \label{fig:ptr_s}
\end{figure}

En el caso de los mensajes de texto se realizó un análisis específico considerando únicamente la cantidad de SMS recibidos, 
utilizando la columna \texttt{OTHER\_NUMBER}. Se identificaron outliers que reflejan un comportamiento anómalo en comparación con los usuarios normales. 
Mientras que la mayoría de los registros presentan un número reducido de mensajes recibidos, algunos casos muestran valores significativamente superiores (ver Cuadro \ref{tab:rec_sms}).

\begin{table}[H]
\centering
\begin{tabular}{|p{3cm}|p{5cm}|}
\hline
\textbf{Variable} & \textbf{Valor} \\
\hline
Media Normal & 2.54 \\
\hline
Media Outlier & 25.96 \\
\hline
Min Outlier & 7 \\
\hline
Max Outlier & 168 \\
\hline
\end{tabular}
\caption{Resumen de valores de la variable mensajes recibidos.}
\label{tab:rec_sms}
\end{table}


\section{Reducción de dimensionalidad y ruido}

Si bien se tienen pocas variables que no deberían suponer un problema al ejecutar los algoritmos clásicos, existe una alta correlación (ver Cuadro \ref{tab:correlaciones}) entre algunas de ellas, de las que se pueden prescindir.

\begin{table}[H]
\centering
\begin{tabular}{|p{4.2cm}|p{4cm}|p{3cm}|}
\hline
\textbf{Variable 1} & \textbf{Variable 2} & \textbf{Correlación} \\
\hline
RATING\_USAGE & ACTUAL\_USAGE & 1.000000 \\
\hline
DEFAULT\_ACCT\_ID & OBJ\_ID & 1.000000 \\
\hline
DEFAULT\_ACCT\_ID & OWNER\_CUST\_ID & 1.000000 \\
\hline
OWNER\_CUST\_ID & OBJ\_ID & 1.000000 \\
\hline
OBJ\_ID & CDR\_ID & 0.913608 \\
\hline
DEFAULT\_ACCT\_ID & CDR\_ID & 0.913608 \\
\hline
OWNER\_CUST\_ID & CDR\_ID & 0.913607 \\
\hline
\end{tabular}
\caption{Correlaciones entre variables seleccionadas}
\label{tab:correlaciones}
\end{table}

Para abordar este problema se proponen dos enfoques con supuestos conceptuales distintos: 

\begin{itemize} 
    \item \textbf{Análisis de Componentes Principales (PCA)}: considera que la información está contenida en la varianza de los datos. 
    \item \textbf{Análisis de Componentes Independientes (ICA)}: asume que la información se encuentra en la independencia estadística de las variables. 
\end{itemize} 

Los resultados muestran que las dos primeras componentes extraídas por PCA e ICA corresponden principalmente a los identificadores de usuario y al consumo, junto con las columnas altamente correlacionadas con ellos.

Finalmente, se observa que tanto PCA como ICA omitieron las variables temporales. Esto resulta coherente con el conjunto de datos analizado, ya que la información disponible se restringe al intervalo comprendido entre las 12:00 a.m. y las 8:00 a.m. del día 1 de octubre de 2025.

\section{Framework de AutoML}

Tradicionalmente, el ciclo de vida de un proyecto de Machine Learning (ML) involucra fases iterativas como:
\begin{itemize}
    \item \textbf{Preprocesamiento y Feature Engineering:} transformación y creación de variables para capturar patrones relevantes.
    \item \textbf{Selección del Modelo:} elección del algoritmo o familia de algoritmos más apropiada.
    \item \textbf{Entrenamiento y Validación:} ajuste de los parámetros del modelo y evaluación preliminar.
    \item \textbf{Afinamiento de Hiperparámetros:} optimización de los parámetros que gobiernan el proceso de aprendizaje.
    \item \textbf{Evaluación Final y Despliegue:} validación en conjuntos de prueba e implementación.
\end{itemize}

La falta de estandarización, derivada de la naturaleza diversa de los problemas abordados por el \gls{ml}, 
junto con el alto costo en \textit{horas de trabajo especializado} y recursos de cómputo, 
ha generado históricamente ineficiencias significativas. Estas se manifiestan en procesos manuales 
propensos a errores humanos, una experimentación limitada por las restricciones de tiempo 
y soluciones con baja reproducibilidad que dificultan su auditoría.


\subsection{El Paradigma AutoML}

El avance de las técnicas englobadas bajo el término \textbf{AutoML} (Automated Machine Learning) ha permitido automatizar gran parte de estas tareas, que antes dependían capitalemente del criterio y la experiencia del especialista (\textit{``ojo del experto''}).

Si bien actualmente no existe un framework universal que, de manera rápida y para el caso general, genere un prototipo de solución óptima para proyectos reales complejos (y teoremas como el \textit{``No Free Lunch''} demuestran que tal herramienta omnipotente no puede existir), la implementación de soluciones AutoML \textit{ad-hoc} persigue objetivos clave:
\begin{enumerate}
    \item \textbf{Reducción del sesgo humano:} minimizar la subjetividad en la selección y configuración de modelos.
    \item \textbf{Aumento de la productividad:} liberar al experto de tareas repetitivas para que se centre en el diseño del problema y la interpretación de resultados.
    \item \textbf{Mejora de la exhaustividad:} explorar de forma sistemática y automática espacios de búsqueda (modelos, hiperparámetros) mucho más amplios.
\end{enumerate}

En aras de estos objetivos y para abordar las necesidades específicas de este proyecto (en particular, el trabajo con \textbf{series de tiempo}), se optó por diseñar e implementar un \textit{mini-framework} de AutoML especializado (ver Figura \ref{fig:fw_aml}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/framework_autoML.jpg}
    \caption{Framework para AutoML.}
    \label{fig:fw_aml}
\end{figure}

\subsection{Arquitectura del Mini-Framework}

El sistema propuesto se estructura en los siguientes componentes modulares:

\begin{itemize}
    \item \textbf{Interfaz Visual de Configuración:} permite al usuario definir los parámetros de alto nivel del \textit{pipeline} AutoML (no los hiperparámetros internos de cada modelo). Desde aquí se seleccionan:
    \begin{itemize}
        \item Conjuntos de características (\textit{features}) a evaluar.
        \item Métricas de evaluación primarias y secundarias (e.g., RMSE, MAE, MAPE).
        \item Restricciones de tiempo y recursos computacionales.
    \end{itemize}
    \item \textbf{Adaptadores de Modelo (\textit{Model Adapters}):} capa de abstracción que unifica la interfaz de entrada/salida para diversos tipos de modelos (estadísticos como ARIMA, basados en árboles, redes neuronales, etc.), facilitando su integración en el \textit{pipeline}.
    \item \textbf{Motor de Búsqueda y Optimización:} componente central que implementa el algoritmo de búsqueda. Opera bajo la hipótesis simplificadora de que el \textit{pipeline} óptimo está compuesto por un \textbf{único modelo} (es decir, no explora automáticamente \textit{ensambles} o \textit{stacks} de modelos). Realiza una \textbf{búsqueda exhaustiva} (o semi-exhaustiva, según espacios muy grandes) sobre el espacio definido de modelos e hiperparámetros, guiada por la métrica objetivo especificada por el usuario.
    \item \textbf{Módulo de Visualización y Comparación:} genera reportes y visualizaciones unificadas (pronóstico vs. real, comparación de métricas entre modelos) que facilitan el análisis comparativo y la toma de decisiones final por parte del experto.
\end{itemize}

Esta arquitectura permite \textbf{eliminar de forma automática variantes de modelo poco efectivas} en las etapas tempranas, filtrando solo las configuraciones más prometedoras para una evaluación detallada. Esto resulta en un \textbf{uso más eficiente del tiempo del experto} y de los recursos computacionales.

\subsection{Resultados y Análisis}

La aplicación del framework al dominio específico de las series de tiempo del proyecto arrojó resultados consistentes con la intuición teórica:

\begin{itemize}
    \item Los modelos más exitosos fueron, en su mayoría, \textbf{modelos estadísticos clásicos} que imponen un sesgo (\textit{inductive bias}) fuerte y apropiado sobre la estructura temporal de los datos (estacionalidad, tendencia, autocorrelación). Estos modelos, al estar bien especificados para el problema, requieren relativamente pocos datos para un ajuste robusto.
    % \item El modelo que \textbf{minimizó el Error Cuadrático Medio Raíz (RMSE)} en el conjunto de validación fue \textbf{(S)ARIMA} (Modelo Autorregresivo Integrado de Medias Móviles, con componentes estacionales). Su rendimiento superior se explica por su capacidad para capturar de manera eficiente la dinámica de \textbf{medias móviles} y la dependencia temporal a corto y largo plazo presentes en los datos.
    \item El proceso AutoML permitió descartar rápidamente enfoques más complejos (como ciertas redes neuronales) que, sin un ajuste muy especializado y mayor volumen de datos, no superaban la robustez y simplicidad de los modelos estadísticos.
\end{itemize}

Además se hizo un pequeño estudio extra para validar que el \textit{feature engineering} no cause ruido en predicciones de largas distancias. Se utilizó otro conjunto de datos para este análisis y se obtuvieron buenos resultados (ver Figura \ref{fig:fet_eng}). Ningún algoritmo tuvo un comportamiento anómalo con respecto a los datos esperados. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/test_feature_engineering.jpg}
    \caption{Predicción usando feature engineering.}
    \label{fig:fet_eng}
\end{figure}

\section{Elección de modelos para predicción}

Utilizando los módulos del \textit{framework} se hicieron predicciones de cada uno de los servicios del estudio. Se tuvieron en cuenta los algoritmos: Random Forest, Gradient Boosting, Regresión Lineal y Ridge Regression.
La métrica utilizada fue la raíz del error cuadrático medio (mientras más pequeña mejor). Para todos los servicios se obtuvo que el mejor algoritmo fue Random Forest, aunque hubo 
pequeñas diferencias en el orden de eficiencia según el servicio.

\subsection{Datos}

En el conjunto de prueba se obtuvo 500\,194\,342.53 de RMSE que puede parecer elevado, pero en este caso, el modelo logra capturar adecuadamente 
la tendencia general del consumo, como se evidencia en la superposición visual entre la serie real y la predicha. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/resultados_datos.png}
    \caption{Proyección de los datos de prueba sobre consumo de datos.}
    \label{fig:pred_d}
\end{figure}

Sin embargo, también se observan momentos de subestimación y sobreestimación, lo que sugiere que aún existe margen de mejora en la precisión del modelo, especialmente en los picos de consumo (ver Figura \ref{fig:pred_d}).

\subsection{Voz}

En la Figura  \ref{fig:pred_v} se observa que se obtuvo 42.08 de RMSE lo cual indica que el modelo logra una predicción
bastante precisa.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/resultados_voz.png}
    \caption{Proyección de los datos de prueba sobre consumo de voz.}
    \label{fig:pred_v}
\end{figure}

La superposición entre la serie real y la predicha muestra una cierta concordancia, lo que no logra capturar las fluctuaciones
notables en el transcurso del tiempo. 

\subsection{SMS}

Visualmente, la línea roja discontinua (predicción) sigue de cerca la trayectoria de la línea negra (real), 
lo que sugiere que el modelo logra capturar con precisión la dinámica del fenómeno observado. 
No se aprecian desviaciones sistemáticas ni errores persistentes, lo que refuerza la interpretación de que el modelo tiene buen ajuste en este intervalo temporal (ver Figura \ref{fig:pred_s}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/resultados_sms.png}
    \caption{Proyección de los datos de prueba sobre consumo de sms.}
    \label{fig:pred_s}
\end{figure}

El RMSE de 4.36 resulta bajo en términos absolutos, lo que evidencia un margen de error reducido y respalda la confiabilidad del modelo para tareas de predicción en este contexto.

\subsection{Predicción de consumo}

Como resultado de todo el análisis realizado se pudieron generar predicciones de todos los servicios en un espacio de tiempo de 30 minutos a partir de que 
finaliza el tiempo registrado en el conjunto de datos inicial.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/consumo_datos.png}
    \caption{Predicción sobre consumo de datos. Muestra una tendencia creciente en los próximos 30 minutos.}
    \label{fig:pred_t_d}
\end{figure}

La Figura \ref{fig:pred_t_d} muestra un aumento abrupto alrededor de las 7:30 am, seguido de una estabilización en valores altos, lo que sugiere un cambio de comportamiento en el tráfico que podría estar asociado a que inicia el horario laboral y la vida en general.
 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/consumo_voz.png}
    \caption{Predicción sobre consumo de voz. Se espera un brusco descenso pero luego se mantiene constante en los próximos 30 minutos.}
    \label{fig:pred_t_v}
\end{figure}

La Figura \ref{fig:pred_t_v} muestra una predicción de consumo de voz con una variación inicial moderada, seguida de una estabilización progresiva en torno a valores medios. El descenso al inicio refleja una leve corrección en el patrón de tráfico, que luego se mantiene relativamente constante, lo que sugiere un comportamiento regular durante el intervalo observado.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/consumo_sms.png}
    \caption{Predicción sobre consumo de sms. Se proyectan variaciones sostenidas en los próximos 30 minutos}
    \label{fig:pred_t_s}
\end{figure}

En la Figura \ref{fig:pred_t_s} se observa una variabilidad moderada, sin saltos abruptos ni tendencias marcadamente crecientes o decrecientes. El comportamiento es relativamente estable, con fluctuaciones suaves que podrían reflejar cambios menores en la actividad de envío de mensajes durante ese periodo. Esto sugiere un patrón de tráfico regular, sin anomalías evidentes.

\section{Conclusiones}

A pesar de contar con pocos datos (no fueron muy representativos porque los registros pertenecían a tan solo 8 horas de un mismo día) se obtuvieron resultados interesantes. 
Se pudo detectar actividades anómalas en el consumo de los distintos servicios: datos, voz y sms. 

Sin embargo, este hecho nunca impuso un problema mayor. Los estudios realizados sobre los datos 
se hicieron de forma genérica con el propósito de que fueran adaptables a otras muestras de datos.

La implementación de un mini-framework de AutoML especializado demostró ser una herramienta valiosa dentro del proyecto. 
No solo agilizó el ciclo de experimentación, sino que también proporcionó una base objetiva y reproducible para la selección del modelo final. 

La experiencia reafirma que, si bien el AutoML no reemplaza al experto, lo potencia al permitirle centrarse en la formulación del problema, el diseño de características de alto nivel y la interpretación de resultados, mientras automatiza la búsqueda extensiva y sistemática en el espacio de soluciones técnicas.

\newpage

\printglossaries

\newpage

% Ejemplo de bibliografía manual
\begin{thebibliography}{99}
\bibitem{DBSCAN} Y. Zhang, X. Li, and J. Wang, ``Outlier Detection in Streaming Data for Telecommunications and Industrial Applications: A Survey'', Electronics, vol. 13, no. 16, p. 3339, 2024. \url{https://doi.org/10.3390/electronics13163339}
\end{thebibliography}

\end{document}
